import numpy as np
import re
import nltk

import logging
logging.basicConfig(level=logging.DEBUG)


class TextGenModel:
    SENT_START_TOKEN = "SENTENCE_START"
    SENT_END_TOKEN = "SENTENCE_END"
    UNKNOWN_TOKEN = "UNKNOWN_TOKEN"
    PAD_TOKEN = "PADDING"

    TYPES = {'delimited': 0,  # characterized by start and end sentence tokens
             'continuous': 1,  # no delimiter tokens
             }

    def __init__(self, model, index_to_word, word_to_index, sent_max_len=30, temperature=1.0,
                 use_embeddings=False, model_type=TYPES['delimited']):
        self.model = model
        self.index_to_word = index_to_word
        self.word_to_index = word_to_index
        self.vocabulary_size = len(word_to_index)
        self.unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]
        self.pad_token_idx = self.word_to_index[self.PAD_TOKEN]

        self.start_token_idx = self.word_to_index.get(self.SENT_START_TOKEN, None)
        self.end_token_idx = self.word_to_index.get(self.SENT_END_TOKEN, None)

        self.sent_max_len = sent_max_len
        self.temperature = temperature
        self.use_embeddings = use_embeddings
        self.model_type = model_type

    def get_sentence(self, sent_min_len, seed=None, seed_max_len = 10):
        """
        Returns a sentence generated by the model.
        Method will loop on generated sentences until requirements are met.
        :param sent_min_len: minimum length acceptable for the sentence
        :param seed: seed text to use for the generation task
        :param seed_max_len: max length used for the seed (operated pre-truncation)
        :return: the generated string
        """
        if seed == '':
            seed = None

        start_sentence = self._generate_start_sentence(seed_max_len, seed)

        sent = None
        while not sent:
            sent = self._generate_sentence(sent_min_len, start_sentence)
        return sent

    def _generate_start_sentence(self, max_len, seed=None):
        """
        Generate sentence based on seed text.
        Sentence should then be feed to the model for the actual text generation task.
        """
        # if we have some seed text, start with that
        if seed:
            idx_sentence = self.transform_sentence(seed, max_len)
        else:
            # if present use start token
            if self.start_token_idx:
                start_token_idx = self.start_token_idx
            # otherwise use random token index
            else:
                start_token_idx = np.random.randint(self.vocabulary_size)
            idx_sentence = np.array([start_token_idx])
        # one hot encode sentence
        if not self.use_embeddings:
            return self.one_hot_encode_sentence(idx_sentence)
        else:
            return idx_sentence

    # TODO improve on prediction, UNKNOWN handling and timeout
    def _generate_sentence(self, min_len, seed_sentence):
        """
        Main procedure for sentence generation.
        :param min_len: minimum length acceptable for the generated sentence
        :param seed_sentence: seed sentence on which to build newly generated text (can be either one-hot or word-index encoded)
        :return:
        """
        # seed_sentence holds all history of words,
        # res_sentence is instead the final resulting response sentence
        res_sentence = []

        # Repeat until stopping criteria are met
        while True:
            # Index from which we will sample is the one corresponding to index of last
            # word in the fed sentence (RNN next predicted word)
            current_idx = max(0, len(seed_sentence) - 1)

            # Make predictions and sample index based on returned probabilities
            words_probs = self._predict_fun(seed_sentence, idx=current_idx)
            sampled_index = TextGenModel.sample_from_prediction(words_probs, temperature=self.temperature)

            # TODO could try some more sampling before throwing away already done work?
            #Skip if sentence is getting too long or we got an unwanted token
            if len(res_sentence) >= self.sent_max_len \
                    or sampled_index == self.unknown_token_idx \
                    or sampled_index == self.pad_token_idx:
                return None

            # Append result to sentences
            res_sentence.append(sampled_index)
            # append also to seed sentence (just index if with embedding, one-hot encoded if without
            if self.use_embeddings:
                seed_sentence = np.append(seed_sentence, [sampled_index], axis=0)
            else:
                seed_sentence = np.append(seed_sentence, [self.one_hot_encode_word(sampled_index)], axis=0)

            # Return if we get an end token and sentence is long enough
            if len(res_sentence) > min_len and (not self.end_token_idx or
                                                        sampled_index == self.end_token_idx):
                return res_sentence

    def _generate_answer(self, min_len, oh_question):
        # Make predictions and sample index based on probs
        answer_probs = self._predict_fun(oh_question)[0]

        # Repeat until we get an end token
        while True:
            idx_sentence = []
            for words_probs in answer_probs:
                sampled_index = self.sample_from_prediction(words_probs, temperature=self.temperature)

                idx_sentence.append(sampled_index)

                # Skip if sentence is getting too long or we got an unknown token
                if sampled_index == self.unknown_token_idx:
                    break
                # Return if we get an end token and sentence is long enough
                if sampled_index == self.end_token_idx and len(idx_sentence) > min_len:
                    return idx_sentence

    # TODO add padding for models who need fixed size input
    def _predict_fun(self, sentence, idx=None):
        """
        Model prediction based on give sentence
        :param sentence: sentence to use (one-hot encoded)
        :param idx: if specified, return only probabilities for such index
        """
        # Give required shape (sample=1, sent_len, voc_size)
        x = np.expand_dims(sentence, 0)
        predictions = self.model.predict(x)
        if not idx is None:
            return predictions[0][idx]
        else:
            return predictions

    @staticmethod
    def sample_from_prediction(predictions, temperature=1.0):
        """
        Sample an index from given prediction (a probability distribution)
        :param predictions:
        :param temperature:
        :return:
        """
        p = np.log(np.asarray(predictions).astype('float64')) / temperature
        p = np.exp(p) / np.sum(np.exp(p))
        try:
            idx = np.argmax(np.random.multinomial(1, p, 1))
        except ValueError as e:
            logging.debug(e)
            return np.random.choice(len(predictions), 1, p=predictions)[0]
        return idx

    def transform_sentence(self, sentence, max_len):
        """
        Takes a sentence as string and transform it as required by the model
        (tokenize, words to index)
        """
        # Tokenize
        # TODO use only spacy
        words = nltk.word_tokenize(sentence)  # consider adding lower
        # Words to index
        idx_sentence = [self.word_to_index.get(w, self.unknown_token_idx) for w in words[-max_len:]]
        return idx_sentence

    def one_hot_encode_sentence(self, sentence):
        """
        Encode each word of the sentence to one-hot representation.
        :param sentence: sentence in words indexes representation
        """
        return np.eye(self.vocabulary_size)[sentence]

    def one_hot_encode_word(self, word_idx):
        """
        Encode word to one-hot representation.
        :param word_idx: word index to encode
        """
        oh_word = np.zeros(self.vocabulary_size)
        if word_idx >= self.vocabulary_size:
            raise Exception("Word index {} is out of range given a vocabulary size of {}"
                            .format(word_idx, self.vocabulary_size))
        else:
            oh_word[word_idx] = 1.0
        return oh_word

    def _sample_word_idx_from(self, words_probs):
        unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]
        sampled_index = unknown_token_idx
        # Sample until a known word is found
        while sampled_index == unknown_token_idx:
            samples = np.random.multinomial(1, words_probs[-1])
            sampled_index = np.argmax(samples)
        return sampled_index

    # prettify raw generated sentence to string
    def pretty_print_sentence(self, sentence, text_max_len=None):
        # if model of type delimited, remove end token
        if self.model_type == self.TYPES['delimited']:
            sentence = sentence[:-1]
        # if specified, remove unwanted tail text
        if text_max_len:
            sentence = sentence[:text_max_len]
        # convert sentence from word_indexes to string, and tries to fix spacing
        words = [self.index_to_word[word_idx].strip() for word_idx in sentence]
        words = [w if re.match(r"[\,!\?\':\.]+|n'", w) else ' ' + w for w in
                 words]
        # return joined words (spaced added before)
        return "".join(words).strip()