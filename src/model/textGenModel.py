import numpy as np
import re
import nltk

import logging
logging.basicConfig(level=logging.DEBUG)

class TextGenModel:
    SENT_START_TOKEN = "SENTENCE_START"
    SENT_END_TOKEN = "SENTENCE_END"
    UNKNOWN_TOKEN = "UNKNOWN_TOKEN"
    PAD_TOKEN = "PADDING"

    def __init__(self, model, index_to_word, word_to_index, sent_max_len=30, temperature=1.0):
        self.model = model
        self.index_to_word = index_to_word
        self.word_to_index = word_to_index
        self.vocabulary_size = len(word_to_index)
        self.start_token_idx = self.word_to_index[self.SENT_START_TOKEN]
        self.unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]
        self.end_token_idx = self.word_to_index[self.SENT_END_TOKEN]
        self.pad_token_idx = self.word_to_index[self.PAD_TOKEN]

        self.sent_max_len = sent_max_len
        self.temperature = temperature

    def get_sentence(self, sent_min_len, seed=None, seed_max_len = 10):
        """
        Returns a sentence generated by the model.
        Method will loop on generated sentences until requirements are met.
        :param sent_min_len: minimum length acceptable for the sentence
        :param seed: seed text to use for the generation task
        :param seed_max_len: max length used for the seed (operated pre-truncation)
        :return: the generated string
        """
        if seed == '':
            seed = None

        start_sentence = self._generate_start_sentence(seed_max_len, seed)

        sent = None
        while not sent:
            sent = self._generate_sentence(sent_min_len, start_sentence)
        return sent

    def _generate_start_sentence(self, max_len, seed=None):
        """
        Generate sentence based on seed text.
        Sentence should then be feed to the model for the actual text generation task.
        """
        # if we have some seed text, start with that
        if seed:
            oh_sentence = self.transform_sentence(seed, max_len)
        # TODO start token or random idx?
        # otherwise just use the start token
        else:
            #rand_idx = np.random.randint(self.vocabulary_size)
            oh_sentence = np.array([self.one_hot_encode_word(self.start_token_idx)])
        return oh_sentence

    def _generate_sentence(self, min_len, oh_sentence):
        """
        Main procedure for sentence generation.
        :param min_len: minimum length acceptable for the generated sentence
        :param oh_sentence: seed sentence on which to build newly generated text (one-hot encoded)
        :return:
        """
        # oh_sentence is one-hot encoded, idx_sentence is word index encoded
        # the former holds all history of words, the latter just the idx for the final sentence
        oh_sentence = oh_sentence
        idx_sentence = []

        # Repeat until stopping criteria are met
        while True:
            # Index from which we will sample is the one corresponding to index of last
            # word in the fed sentence (RNN next predicted word)
            current_idx = max(0, len(oh_sentence) - 1)

            # Make predictions and sample index based on returned probabilities
            words_probs = self._predict_fun(oh_sentence, idx=current_idx)
            sampled_index = TextGenModel.sample_from_prediction(words_probs, temperature=self.temperature)
            #sampled_word = self.index_to_word[sampled_index]

            # TODO could try some more sampling before throwing away already done work?
            #Skip if sentence is getting too long or we got an unwanted token
            if len(idx_sentence) >= self.sent_max_len \
                    or sampled_index == self.unknown_token_idx \
                    or sampled_index == self.pad_token_idx:
                return None

            # Append result to sentences
            idx_sentence.append(sampled_index)
            oh_sentence = np.append(oh_sentence, [self.one_hot_encode_word(sampled_index)], axis=0)

            # Return if we get an end token and sentence is long enough
            if len(idx_sentence) > min_len and sampled_index == self.end_token_idx:
                return idx_sentence

        return idx_sentence

    def _generate_answer(self, min_len, oh_question):
        # Make predictions and sample index based on probs
        answer_probs = self._predict_fun(oh_question)[0]

        # Repeat until we get an end token
        while True:
            idx_sentence = []
            for words_probs in answer_probs:
                sampled_index = self.sample_from_prediction(words_probs, temperature=self.temperature)

                idx_sentence.append(sampled_index)

                # Skip if sentence is getting too long or we got an unknown token
                if sampled_index == self.unknown_token_idx:
                    break
                # Return if we get an end token and sentence is long enough
                if sampled_index == self.end_token_idx and len(idx_sentence) > min_len:
                    return idx_sentence

    def _predict_fun(self, sentence, idx=None):
        """
        Model prediction based on give sentence
        :param sentence: sentence to use (one-hot encoded)
        :param idx: if specified, return only probabilities for such index
        """
        # Give required shape (sample=1, sent_len, voc_size)
        x = np.reshape(sentence, (1, sentence.shape[0], sentence.shape[1]))
        predictions = self.model.predict(x)
        if not idx is None:
            return predictions[0][idx]
        else:
            return predictions

    @staticmethod
    def sample_from_prediction(prediction, temperature=1.0):
        """
        Sample an index from given prediction (a probability distribution)
        :param prediction:
        :param temperature:
        :return:
        """
        p = np.log(prediction) / temperature
        p = np.exp(p) / np.sum(np.exp(p))
        try:
            idx = np.argmax(np.random.multinomial(1, p, 1))
        except ValueError as e:
            logging.debug(e)
            return np.random.choice(len(prediction), 1, p=prediction)[0]
        #idx = np.random.choice(len(prediction), 1, p=prediction)[0]
        return idx

    def transform_sentence(self, sentence, max_len):
        """
        Takes a sentence as string and transform it as required by the model
        (tokenize, words to index and one-hot encoding)
        """
        # Tokenize
        words = nltk.word_tokenize(sentence)  # consider adding lower
        # Words to index
        idx_sentence = [self.word_to_index.get(w, self.unknown_token_idx) for w in words[-max_len:]]
        # one-hot encoding
        oh_sentence = self.one_hot_encode_sentence(idx_sentence)
        return oh_sentence

    def one_hot_encode_sentence(self, sentence):
        """
        Encode each word of the sentence to one-hot representation.
        :param sentence: sentence in words indexes representation
        """
        return np.eye(self.vocabulary_size)[sentence]

    def one_hot_encode_word(self, word_idx):
        """
        Encode word to one-hot representation.
        :param word_idx: word index to encode
        """
        oh_word = np.zeros(self.vocabulary_size)
        if word_idx >= self.vocabulary_size:
            raise Exception("Word index {} is out of range given a vocabulary size of {}"
                            .format(word_idx, self.vocabulary_size))
        else:
            oh_word[word_idx] = 1.0
        return oh_word

    def _sample_word_idx_from(self, words_probs):
        unknown_token_idx = self.word_to_index[self.UNKNOWN_TOKEN]
        sampled_index = unknown_token_idx
        # Sample until a known word is found
        while sampled_index == unknown_token_idx:
            samples = np.random.multinomial(1, words_probs[-1])
            sampled_index = np.argmax(samples)
        return sampled_index

    # Converts sentence from word_indexes to string, and tries to fix spacing
    def pretty_print_sentence(self, sentence, skip_last=False):
        if skip_last:
            sentence = sentence[:-1]
        words = [self.index_to_word[word_idx].strip() for word_idx in sentence]
        words = [w if re.match(r"[\,!\?\':\.]+|n'", w) else ' ' + w for w in
                 words]
        return "".join(words).strip()