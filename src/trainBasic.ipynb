{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../conversation-analyzer/src')\n",
    "\n",
    "import RNNNumpy\n",
    "from RNNNumpy import RNNNumpy\n",
    "import util.io as mio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reload() argument must be module",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f55c82d6e828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNNumpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\IBM_ADMIN\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mreload\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reload() argument must be module\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__spec__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: reload() argument must be module"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(RNNNumpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 1000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading conversation and load messages\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading conversation and load messages\")\n",
    "messages, senders = mio.parseMessagesFromFile(\"../../conversation-analyzer/src/resources/unittest/test_plotting.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2445 sentences.\n",
      "Found 4042 unique words tokens.\n",
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'pursuing' and appeared 2 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START afar prairie overhead under last all master had ? SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'afar', 'prairie', 'UNKNOWN_TOKEN', 'under', 'last', 'all', 'master', 'had', '?', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "# Split full message text into sentences\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(m.text.lower()) for m in messages])\n",
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"{} {} {}\".format(sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed {} sentences.\".format(len(sentences)))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found {} unique words tokens.\".format(len(word_freq.items())))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size {}.\".format(vocabulary_size))\n",
    "print(\"The least frequent word in our vocabulary is '{}' and appeared {} times.\".format(vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '{}'\".format(sentences[0]))\n",
    "print(\"\\nExample sentence after Pre-processing: '{}'\".format(tokenized_sentences[0]))\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1000)\n",
      "[[ 0.00099956  0.00100781  0.00098837 ...,  0.00099714  0.00099506\n",
      "   0.0009948 ]\n",
      " [ 0.00100628  0.00098631  0.0009982  ...,  0.00100238  0.00101787\n",
      "   0.00101364]\n",
      " [ 0.00098735  0.00101889  0.00097926 ...,  0.00100335  0.00101553\n",
      "   0.00099698]\n",
      " ..., \n",
      " [ 0.00098017  0.00100373  0.0010062  ...,  0.0009999   0.00100029\n",
      "   0.00097951]\n",
      " [ 0.00102032  0.00097848  0.00099343 ...,  0.00100486  0.00100042\n",
      "   0.00100318]\n",
      " [ 0.00099547  0.0009793   0.00100029 ...,  0.00100006  0.00098851\n",
      "   0.00099608]]\n",
      "(7,)\n",
      "[978 229 732 506 995 336 898]\n",
      "Expected Loss for random predictions: 6.907755278982137\n",
      "Actual loss: 6.907880034402725\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)\n",
    "\n",
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "\n",
    "print(\"Expected Loss for random predictions: {}\".format(np.log(vocabulary_size)))\n",
    "print(\"Actual loss: {}\".format(model.calculate_loss(X_train[:1000], y_train[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter %s passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter %s passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter %s passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 4.12 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.numpy_sdg_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-09 10:19:46: Loss after num_examples_seen=0 epoch=0: 6.9084991829956435\n",
      "2016-08-09 10:19:47: Loss after num_examples_seen=100 epoch=1: 6.894771144381747\n",
      "2016-08-09 10:19:48: Loss after num_examples_seen=200 epoch=2: 6.821016469761002\n",
      "2016-08-09 10:19:48: Loss after num_examples_seen=300 epoch=3: 5.109012649166927\n",
      "2016-08-09 10:19:49: Loss after num_examples_seen=400 epoch=4: 4.828119925417974\n",
      "2016-08-09 10:19:49: Loss after num_examples_seen=500 epoch=5: 4.697800108071239\n",
      "2016-08-09 10:19:50: Loss after num_examples_seen=600 epoch=6: 4.616120968698019\n",
      "2016-08-09 10:19:51: Loss after num_examples_seen=700 epoch=7: 4.5588475587105854\n",
      "2016-08-09 10:19:51: Loss after num_examples_seen=800 epoch=8: 4.515708344602935\n",
      "2016-08-09 10:19:52: Loss after num_examples_seen=900 epoch=9: 4.480891876223427\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train_with_sgd(X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall go of our , sits captain marriage the and , see corpse of marriage nor perhaps . by i master\n",
      "globe and have open to , . are ,\n",
      "2 large all a i d , in or her a journey not is of love ' - as in all are will does families full now remain as , , his the\n",
      "can under -- rivers you not all , the 2 , . shall sits the the his , - so from those not one o d\n",
      "art coming , ' foot children o and\n",
      "poems those shining it you , , the of ,\n",
      "swear ever of in , young the from pennants of faith by , the his lines ; and see -\n",
      "each wealth the at , thy forth old in here\n",
      "president side for forty thou bed you - others shade sons will ' - of , arms , marks proud -\n",
      "from whose bend in fields and real on thousand way in lost is is face and all\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        #print(next_word_probs)\n",
    "        #print( np.array(next_word_probs).shape)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
