{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../conversation-analyzer/src')\n",
    "\n",
    "import RNNNumpy\n",
    "from RNNNumpy import RNNNumpy\n",
    "import util.io as mio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import RNNNumpy\n",
    "importlib.reload(RNNNumpy)\n",
    "from RNNNumpy import RNNNumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 1000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading conversation and load messages\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading conversation and load messages\")\n",
    "messages, senders = mio.parseMessagesFromFile(\"../../conversation-analyzer/src/resources/unittest/test_plotting.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2445 sentences.\n",
      "Found 4042 unique words tokens.\n",
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'pensive' and appeared 2 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START afar prairie overhead under last all master had ? SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'afar', 'prairie', 'UNKNOWN_TOKEN', 'under', 'last', 'all', 'master', 'had', '?', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "# Split full message text into sentences\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(m.text.lower()) for m in messages])\n",
    "# Append SENTENCE_START and SENTENCE_END\n",
    "sentences = [\"{} {} {}\".format(sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed {} sentences.\".format(len(sentences)))\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found {} unique words tokens.\".format(len(word_freq.items())))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size {}.\".format(vocabulary_size))\n",
    "print(\"The least frequent word in our vocabulary is '{}' and appeared {} times.\".format(vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '{}'\".format(sentences[0]))\n",
    "print(\"\\nExample sentence after Pre-processing: '{}'\".format(tokenized_sentences[0]))\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1000)\n",
      "[[ 0.00099956  0.00100781  0.00098837 ...,  0.00099714  0.00099506\n",
      "   0.0009948 ]\n",
      " [ 0.00099746  0.00099528  0.00099504 ...,  0.00102788  0.00100813\n",
      "   0.0010053 ]\n",
      " [ 0.000979    0.00102047  0.00096854 ...,  0.00099184  0.00101241\n",
      "   0.00100023]\n",
      " ..., \n",
      " [ 0.00098321  0.00100457  0.00100379 ...,  0.00099772  0.00099781\n",
      "   0.00097394]\n",
      " [ 0.0009994   0.00099131  0.00097729 ...,  0.0009951   0.00100713\n",
      "   0.00101708]\n",
      " [ 0.00100158  0.00099269  0.00099028 ...,  0.00099766  0.00101459\n",
      "   0.00098505]]\n",
      "(7,)\n",
      "[978 548 680 506 272 217 549]\n",
      "Expected Loss for random predictions: 6.907755278982137\n",
      "Actual loss: 6.908071141028713\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)\n",
    "\n",
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "\n",
    "print(\"Expected Loss for random predictions: {}\".format(np.log(vocabulary_size)))\n",
    "print(\"Actual loss: {}\".format(model.calculate_loss(X_train[:1000], y_train[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter %s passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter %s passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter %s passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 16.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.numpy_sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-11 10:32:44: Loss after num_examples_seen=0 epoch=0: 6.908077568797546\n",
      "2016-08-11 10:33:45: Loss after num_examples_seen=2445 epoch=1: 4.574073072159846\n",
      "2016-08-11 10:34:52: Loss after num_examples_seen=4890 epoch=2: 4.529997351523457\n",
      "2016-08-11 10:36:09: Loss after num_examples_seen=7335 epoch=3: 4.505689009375079\n",
      "2016-08-11 10:37:19: Loss after num_examples_seen=9780 epoch=4: 4.491194958410326\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train_with_sgd(X_train, y_train, nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        #print(next_word_probs)\n",
    "        #print( np.array(next_word_probs).shape)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print(\" \".join(sent))\n",
    "    print(\"###########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
