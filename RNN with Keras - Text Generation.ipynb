{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Intro](#Intro)\n",
    "* [Data loading and preprocessing](#Data-loading-and-preprocessing)\n",
    "* [Model training and evaluation](#Model-training-and-evaluation)\n",
    "* [Text Generations](#Text-Generations)\n",
    "* [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview of text generation with Recurrent Neural Networks (RNN) using Keras.\n",
    "We are going to build and train different models that will be able to generate new pieces of text for different contexts (e.g. motivational quotes, jokes, proverbs, narrative, conversations, Q/A).\n",
    "\n",
    "The code will guide in all the steps necessary for this task, and it's accompanied by technical descriptions as well as external references to go  deeper into the subject. See also the README.md file contained in this repository.\n",
    "\n",
    "Some of the dataset used I got directly from [this repository](https://github.com/svenvdbeukel/Short-text-corpus-with-focus-on-humor-detection) or [here for fortune cookies galore](https://github.com/ianli/fortune-cookies-galore). I will also try to include the trained models data in my repository.\n",
    "You can easily adapt the code for any new kind of dataset you want to experiment with. If you have any doubts or suggestions, feel free to contact me directly, and be sure to share you results if you play with the code on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T08:53:09.220317",
     "start_time": "2017-10-04T08:53:07.561222Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic libraries import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Plotting\n",
    "%matplotlib notebook\n",
    "\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Add system path local modules\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model.textGenModel import TextGenModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to load our dataset and preprocess it such it can be fed to the model. Notice we are working at word level. Moving to character level would require some adjustment to the process and overall model.\n",
    "These are the common steps you should follow to create your training data from your original dataset:\n",
    "* sentence segmentation (if you don't have already separate individual sentences)\n",
    "* sentence tokenization (from sentence to list of words)\n",
    "* add start and end tokens\n",
    "* generate words indexing\n",
    "* pad sequences (pad or truncate sentences to fixed length)\n",
    "* one-hot encode (if you are not going to use an embedding layer in the keras model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:49.616479",
     "start_time": "2017-09-17T11:22:49.596478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset with pickle\n",
    "corpus_name = \"short_oneliners\"\n",
    "with open(\"resources/short_oneliners.pickle\", 'rb') as f: #binary mode (b) is required for pickle\n",
    "    dataset = pickle.load(f, encoding='utf-8') #our dataset is simply a list of string\n",
    "print('Loaded {} sentences\\nExample: \"{}\"'.format(len(dataset), dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:51.049561",
     "start_time": "2017-09-17T11:22:51.043561Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constant token and params for our models\n",
    "START_TOKEN = \"SENTENCE_START\"\n",
    "END_TOKEN = \"SENTENCE_END\"\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "PADDING_TOKEN = \"PADDING\"\n",
    "\n",
    "vocabulary_size = 5000\n",
    "sent_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:53.814719",
     "start_time": "2017-09-17T11:22:51.511588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# work tokenization for each sentence, while adding start and end tokens\n",
    "sentences = [[START_TOKEN] + nltk.word_tokenize(entry.lower()) + [END_TOKEN] for entry in dataset]\n",
    "print('Example: {}'.format(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:54.860779",
     "start_time": "2017-09-17T11:22:54.849779Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates index_to_word and word_to_index mappings, given the data and a max vocabulary size\n",
    "def get_words_mappings(tokenized_sentences, vocabulary_size):\n",
    "    # we can rely on nltk to quickly get the most common words, and then limit our vocabulary to the specified size\n",
    "    frequence = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    vocab = frequence.most_common(vocabulary_size)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    # Add padding for index 0\n",
    "    index_to_word.insert(0, PADDING_TOKEN)\n",
    "    # Append unknown token (with index = vocabulary size + 1)\n",
    "    index_to_word.append(UNKNOWN_TOKEN)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "    return index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:57.668940",
     "start_time": "2017-09-17T11:22:57.439927Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get mappings and update vocabulary size\n",
    "index_to_word, word_to_index = get_words_mappings(sentences, vocabulary_size)\n",
    "vocabulary_size = len(index_to_word)\n",
    "print(\"Vocabulary size = \" + str(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:58.158968",
     "start_time": "2017-09-17T11:22:58.091964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate training data by converting tokenized sentenced to indexes (and replacing unknown words)\n",
    "train_size = min(len(sentences), 100000)\n",
    "train_data = [[word_to_index.get(w,word_to_index[UNKNOWN_TOKEN])  for w in sent] for sent in sentences[:train_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:22:59.775060",
     "start_time": "2017-09-17T11:22:59.703056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sentences to fixed lenght (pad with 0s if shorter, truncate if longer)\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=sent_max_len, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick and dirty way to one-hot encode our training data, not needed if using embeddings\n",
    "#X_train = np.asarray([np.eye(vocabulary_size)[idx_sentence[:-1]] for idx_sentence in train_data])\n",
    "#y_train = np.asarray([np.eye(vocabulary_size)[idx_sentence[1:]] for idx_sentence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:23:06.814463",
     "start_time": "2017-09-17T11:23:06.807463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training data for rnn: \n",
    "# input is sentence truncated from last word, output is sentence truncated from first word\n",
    "X_train = train_data[:,:-1]\n",
    "y_train = train_data[:,1:]\n",
    "#X_train = X_train.reshape([X_train.shape[0], X_train.shape[1], 1])\n",
    "y_train = y_train.reshape([y_train.shape[0], y_train.shape[1], 1]) # needed cause out timedistributed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:23:07.530504",
     "start_time": "2017-09-17T11:23:07.525504Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if expected shapes (samples, sentence length, ?)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define a RNN model architecture, train it on our data, and eventually save the results for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T11:30:42.798544",
     "start_time": "2017-09-17T11:30:42.790543Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:38:38.862781",
     "start_time": "2017-09-17T14:38:38.858781Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model and parameters\n",
    "hidden_size = 512\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:38:40.386868",
     "start_time": "2017-09-17T14:38:38.995788Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model with embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, mask_zero=True))\n",
    "# add batch norm\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(hidden_size, return_sequences=True, activation='relu'))\n",
    "model.add(TimeDistributed(Dense(vocabulary_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic single-layer model\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(hidden_size, input_shape=(None, vocabulary_size), return_sequences=True))\n",
    "#model.add(TimeDistributed(Dense(vocabulary_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-21T10:48:49.347763",
     "start_time": "2017-04-21T10:48:48.487714"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need time distributes for embedding?\n",
    "#model.add(TimeDistributed(\n",
    "#        Embedding(vocabulary_size, output_dim=hidden_size, mask_zero=True, input_length=sent_max_len-1),\n",
    "#          input_shape=(sent_max_len-1, 1), input_dtype='int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:38:42.230973",
     "start_time": "2017-09-17T14:38:42.223973Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:38:45.359152",
     "start_time": "2017-09-17T14:38:45.281148Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recompile also if you just want to keep training a model just loaded from memory\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:13.041895",
     "start_time": "2017-09-17T14:38:51.094480Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "# you might want to train several times on few epoch, observe how loss and metrics vary\n",
    "# and possibly tweak batch size and learning rate\n",
    "num_epoch = 2\n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, epochs=num_epoch, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-28T15:19:07.856446",
     "start_time": "2017-04-28T15:19:06.793385"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export model (architecture)\n",
    "model_path = \"resources/models/{}_vocab_{}.json\".format(corpus_name, vocabulary_size)\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as f:\n",
    "    f.write(model_json)\n",
    "\n",
    "# export model weights\n",
    "weights_path = \"resources/models/{}_epoch_{}.hdf5\".format(corpus_name, 40)\n",
    "model.save_weights(weights_path)\n",
    "\n",
    "# export word indexes\n",
    "index_to_word_path = 'resources/models/{}_idxs_vocab{}.txt'.format(corpus_name, vocabulary_size)\n",
    "with open(index_to_word_path, \"wb\") as f:\n",
    "    pickle.dump(index_to_word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generation part I am relying on a generic utility class (*TextGenModel*) included in this repo.\n",
    "\n",
    "Main operations for this step are:\n",
    "* load previously trained model\n",
    "* instantiate class with the model and model configuration (e.g. temperature, sent max length)\n",
    "* generate new text with target class\n",
    "* prettify generated text\n",
    "\n",
    "For the text generation task a seed sentence can be provided, plus additional requirements on text length.\n",
    "The class will then internally take care of predicting word after word until some criteria are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-28T12:28:15.797474",
     "start_time": "2017-04-28T12:28:15.474456"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"resources/models/jokes_vocab_5002.json\"\n",
    "weights_path = \"resources/models/jokes_epoch_20.hdf5\"\n",
    "# Load previously saved model\n",
    "with open(model_path, 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "# Load weights into model\n",
    "model.load_weights(weights_path)\n",
    "# Load word indexes\n",
    "with open(index_to_word_path, 'rb') as f:\n",
    "    index_to_word = pickle.load(f, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T15:00:13.675840",
     "start_time": "2017-09-17T14:59:55.306789Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# instantiate generation class on our data\n",
    "text_gen = TextGenModel(model, index_to_word, word_to_index, sent_max_len=sent_max_len, \n",
    "                                    temperature=1.0,\n",
    "                                    use_embeddings=True)\n",
    "# generate N new sentences\n",
    "n_sents = 10\n",
    "original_sentences = [entry.lower() for entry in dataset]\n",
    "for _ in range(n_sents):\n",
    "    res = text_gen.pretty_print_sentence(text_gen.get_sentence(15), skip_last=True)\n",
    "    if res in original_sentences:\n",
    "        print(\"* {} SAME\".format(res))\n",
    "    else:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to polish the code in this repository and generalize it even further, such that there is a clear separation between text generation and models training. The idea is that for the latter I can keep experimenting with different techniques and tools, and generate different models, while with the former I can provide a reusable text-generation interface for a multitude of use-cases.\n",
    "\n",
    "I would exactly be interested to see more diffuse and creative usage of text generation: from purely artistic tasks, to personal optimization ones (e.g. text suggestion and check), passing through a bit more automation for all other relevant scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:neural-networks]",
   "language": "python",
   "name": "conda-env-neural-networks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "874px",
    "left": "0px",
    "right": "1693px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "none",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
