{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Intro\" data-toc-modified-id=\"Intro-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intro</a></span></li><li><span><a href=\"#Data-Loading-and-Preprocessing\" data-toc-modified-id=\"Data-Loading-and-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Loading and Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Level-Text\" data-toc-modified-id=\"Sentence-Level-Text-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Sentence Level Text</a></span></li><li><span><a href=\"#Continuous-Text\" data-toc-modified-id=\"Continuous-Text-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Continuous Text</a></span></li><li><span><a href=\"#Load-Word-Embeddings\" data-toc-modified-id=\"Load-Word-Embeddings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Load Word-Embeddings</a></span></li></ul></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoder-Decoder-Model\" data-toc-modified-id=\"Encoder-Decoder-Model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Encoder-Decoder Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-Example\" data-toc-modified-id=\"Keras-Example-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span><a href=\"https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\" target=\"_blank\">Keras Example</a></a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Export-Model\" data-toc-modified-id=\"Export-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Export Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-Basic-Export\" data-toc-modified-id=\"Keras-Basic-Export-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Keras Basic Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-Import\" data-toc-modified-id=\"Keras-Import-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Keras Import</a></span></li></ul></li><li><span><a href=\"#TF-Serving-Export\" data-toc-modified-id=\"TF-Serving-Export-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>TF Serving Export</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-Serving-Client\" data-toc-modified-id=\"TF-Serving-Client-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>TF Serving Client</a></span></li></ul></li></ul></li><li><span><a href=\"#Text-Generation\" data-toc-modified-id=\"Text-Generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Text Generation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#TF-Serving-Client\" data-toc-modified-id=\"TF-Serving-Client-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>TF Serving Client</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook builds on the *RNN with Keras - Text Generation* intro notebook and explores more advanced techniques, configurations and optimization for the text generation task. We want to consider a text generation task as general as possible, with the capability of using multiple models of different nature (e.g. narrative, scientific, code, poetry, conversational) for a variety of uses (e.g. writing hints, chatbots, \"QA\").\n",
    "\n",
    "We are here going to make heavy use of the code and utilities already present in the repository containing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# Keras and Tensorflow\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector, Input\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reference local code\n",
    "import os\n",
    "import sys\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "sys.path.append(join(os.getcwd(), 'src'))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import preprocessing\n",
    "from model.textGenModel import TextGenModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_folder = join(str(Path.home()), \"Documents/datasets/\")\n",
    "model_data_folder = join(str(Path.home()), \"Documents/models/tf_rnn_{}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "For the moment I personally recognized two distinct types of task, which are defined by our requirements in terms on the generated text, and that in turns define different ways to pre/post-process the content.\n",
    "\n",
    "Notice that this distinction might not follow more formal standards defined by the online community regarding the task, I simply found it useful as for the current state of my requirements and understanding.\n",
    "\n",
    "**Q&A (or Sentence Level)**\n",
    "\n",
    "Here we expect to provide a string and get one which relates to it on a semantic level, but that it is also self-contained, meaning that it makes sense by itself, and possibly follows common rules of sentence structure (e.g. start and end tokens).\n",
    "\n",
    "This exactly reflects a question answering scenario, or a more general conversational one (consider a chatbot for example).\n",
    "\n",
    "**Continuous**\n",
    "\n",
    "Here we might provide a seed string, and expect an arbitrary length response that simply follows the \"narrative flow\" of the seed. We are not interested in it to be self-contained.\n",
    "\n",
    "Implications: no start/end tokens, start/end is mostly defined by language syntax (e.g. upper case, punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant token and params for our models\n",
    "START_TOKEN = \"SENTENCE_START\"\n",
    "END_TOKEN = \"SENTENCE_END\"\n",
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "PADDING_TOKEN = \"PADDING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Text\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "sent_max_len = 20\n",
    "corpus_name = 'bible'\n",
    "model_data_folder = model_data_folder.format(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(training_data_folder, \"{}.txt\".format(corpus_name)),\n",
    "         'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text \n",
    "#tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "corpus_tokens = [token.orth_ for token in nlp(corpus_text)]\n",
    "print('Example tokenized excerpt: {}'.format(corpus_tokens[10:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mappings and update vocabulary size\n",
    "index_to_word, word_to_index = preprocessing.get_words_mappings(\n",
    "                                        [corpus_tokens], #cause a list of sentences is expected\n",
    "                                        vocabulary_size)\n",
    "vocabulary_size = len(index_to_word)\n",
    "print(\"Vocabulary size = \" + str(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens to indexes (and replacing unknown words)\n",
    "train_data = [word_to_index.get(w, word_to_index[UNKNOWN_TOKEN]) \n",
    "              for w in corpus_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "# for the \"continuous\" case we split here into sentences\n",
    "# for which output one is 1-shifted to the right (successive words)\n",
    "remainer = len(train_data)%sent_max_len\n",
    "X_train = np.array([train_data[i:i+sent_max_len] \n",
    "                    for i in range(0, len(train_data)-remainer, sent_max_len)])\n",
    "Y_train = np.array([train_data[i:i+sent_max_len] \n",
    "                    for i in range(1, len(train_data)-remainer, sent_max_len)])\n",
    "#X_train = np.expand_dims(X_train, -1)\n",
    "Y_train = np.expand_dims(Y_train, -1) # needed cause out timedistributed layer\n",
    "\n",
    "print(\"Example train input sentence: {}\".format(X_train[0]))\n",
    "print(\"and related output = {}\".format(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if expected shapes (samples, sentence length, ?)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export word indexes\n",
    "index_to_word_path = join(model_data_folder,\n",
    "                          '{}_idxs_vocab_{}.txt'.format(corpus_name, vocabulary_size))\n",
    "with open(index_to_word_path, \"wb\") as f:\n",
    "    pickle.dump(index_to_word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word-Embeddings\n",
    "Notice that with the following approach we are considering only the vocabulary of our training dataset.\n",
    "An improvements on this side would be to reinstantiate the model for testing including the entirety of the original word-embeddings, such that we can leverage also input words not present in our training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "embeddings = preprocessing.load_embeddings(join(training_data_folder, \"glove\", \"glove.6B.100d.txt\"),\n",
    "                                          word_to_index.keys())\n",
    "embeddings_matrix = preprocessing.get_embeddings_matrix(embeddings,\n",
    "                                                       word_to_index,\n",
    "                                                        embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "We first consider a basic LSTM with embedding input layer.\n",
    "Initially we will learn the embedding for scratch, and consider a many-to-many model (that is, sequence-in and sequence-out).\n",
    "\n",
    "Possible improvements and stuff to try:\n",
    "* try deep LSTM\n",
    "* include attention\n",
    "\n",
    "[TODO] Add considerations about stateful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "hidden_size = 512\n",
    "#embedding_size = 128 # already defined when loading word-embeddings\n",
    "batch_size = 64\n",
    "stateful = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, \n",
    "                    #batch_input_shape=(batch_size, sent_max_len), # needed in case of stateful model\n",
    "                    mask_zero=True,\n",
    "                    weights=[embeddings_matrix], # use of pretained embeddings\n",
    "                    trainable=False))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(TimeDistributed(Flatten())) # not needed if proper batch_input_shape specified before\n",
    "model.add(LSTM(hidden_size, \n",
    "               return_sequences=True, \n",
    "               stateful=stateful, # if stateful model, remember to avoid batches shuffling during training\n",
    "              ))#activation='relu')) ??easily getting loss=nan if using RELU\n",
    "model.add(TimeDistributed(Dense(vocabulary_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of trained epoch (for when we rerun a cell)\n",
    "trained_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 1\n",
    "model.fit(X_train[:100000], # for stateful we need \n",
    "          Y_train[:100000], # number of samples divisible by the batch size\n",
    "          epochs=num_epoch, \n",
    "          batch_size=batch_size, \n",
    "          shuffle=not stateful # don't shuffle if stateful \n",
    "         )\n",
    "trained_epochs += num_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "model.fit(X_train, Y_train, \n",
    "          epochs=num_epoch, \n",
    "          batch_size=batch_size, \n",
    "          shuffle=not stateful # don't shuffle if stateful \n",
    "         )\n",
    "trained_epochs += num_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Encoder-Decoder Model\n",
    "Encoder-decoder model. Sometime also called Sequence-to-Sequence, which however can also refer purely to the type of correspondence between input and output.\n",
    "\n",
    "[TODO] Basic description of the model\n",
    "\n",
    "Possible improvements and stuff to try:\n",
    "* include attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size,\n",
    "                    mask_zero=True,\n",
    "                    weights=[embeddings_matrix], # use of pretained embeddings\n",
    "                    trainable=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(hidden_size, return_sequences=False))\n",
    "model.add(RepeatVector(sent_max_len)) #??Difference between RepeatVector and return_sequences\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocabulary_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# keep track of trained epoch (for when we rerun a cell)\n",
    "trained_epochs = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 1\n",
    "model.fit(X_train[:-(len(X_train)%batch_size)], # for stateful we need \n",
    "          Y_train[:-(len(X_train)%batch_size)], # number of samples divisible by the batch size\n",
    "          epochs=num_epoch, \n",
    "          batch_size=batch_size, \n",
    "          shuffle=not stateful # don't shuffle if stateful \n",
    "         )\n",
    "trained_epochs += num_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [Keras Example](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "#encoder_inputs = Embedding(vocabulary_size, embedding_size, \n",
    "#                    mask_zero=True)\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(vocabulary_size, embedding_size)(encoder_inputs)\n",
    "encoder = LSTM(hidden_size, \n",
    "               return_state=True,\n",
    "               return_sequences=True\n",
    "              )\n",
    "#model.add(BatchNormalization())\n",
    "encoder_outputs = encoder(x)\n",
    "encoder_states = [encoder_outputs[1], encoder_outputs[2]]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(vocabulary_size, embedding_size)(decoder_inputs)\n",
    "decoder = LSTM(hidden_size, \n",
    "               return_sequences=True\n",
    "              )\n",
    "decoder_outputs = decoder(x, initial_state=encoder_states)\n",
    "decoder_outputs = TimeDistributed(Dense(vocabulary_size, activation='softmax'))(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Img](https://pbs.twimg.com/media/DK2eRl3V4AAFBWx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Informal evaluation. Use the trained model to generate N new sentences and print them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# instantiate generation class on our data\n",
    "text_gen = TextGenModel(model, index_to_word, word_to_index, sent_max_len=sent_max_len, \n",
    "                                    temperature=1.0,\n",
    "                                    use_embeddings=True)\n",
    "# generate N new sentences\n",
    "n_sents = 10\n",
    "print(\"Epoch {}\".format(trained_epochs))\n",
    "for _ in range(n_sents):\n",
    "    res = text_gen.pretty_print_sentence(text_gen.get_sentence(15))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model\n",
    "Explores both the common Keras export and Tensorflow serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Basic Export\n",
    "Export model architecture and weights such that it can be easily reinstantiate for testing or further training.\n",
    "\n",
    "Notice that word-to-index data is required if we want to reuse the model, but we covered that step already during the preprocessing phase.\n",
    "\n",
    "Note also that we could enable automatic checkpoint saving into the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = join(model_data_folder, 'checkpoints')\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "# export model (architecture)\n",
    "model_path = join(checkpoints_dir, \n",
    "                  \"base_voc_{}.json\".format(vocabulary_size))\n",
    "model_json = model.to_json()\n",
    "with open(model_path, \"w\") as f:\n",
    "    f.write(model_json)\n",
    "\n",
    "# export model weights\n",
    "weights_path = join(checkpoints_dir, \n",
    "                    \"glove_voc_{}_epoch_{}.hdf5\".format(vocabulary_size, \n",
    "                                                      trained_epochs))\n",
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "K.set_session(sess)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.set_learning_phase(False) # see https://github.com/fchollet/keras/issues/2310\n",
    "\n",
    "# Load previously saved model\n",
    "with open(join(model_data_folder, 'checkpoints', 'base_voc_8002.json'), 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "# Load weights into model\n",
    "model.load_weights(join(model_data_folder, 'checkpoints', 'base_voc_8002_epoch_23.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Serving Export\n",
    "Export model for tf-serving relying on `SavedModelBuilder`. The exported model can then be easily \"passed\" to a tf-serving server instance for consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"4\"\n",
    "export_dir = join(model_data_folder, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input)\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init builder\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "# Define signature (set of inputs and outputs for the graph)\n",
    "prediction_signature = (\n",
    "    tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs={'inputs': tf.saved_model.utils.build_tensor_info(model.input)},\n",
    "        outputs={'outputs': tf.saved_model.utils.build_tensor_info(model.output)},\n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add meta-graph (dataflow graph, variables, assets, and signatures) \n",
    "# to the builder\n",
    "#with K.get_session() as sess: #avoid because it causes the session to be closed\n",
    "sess = K.get_session()\n",
    "K.set_learning_phase(False) # see https://github.com/fchollet/keras/issues/2310\n",
    "builder.add_meta_graph_and_variables(\n",
    "    sess=sess,\n",
    "    tags=[tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        'predict' : prediction_signature\n",
    "    }\n",
    "    #legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    ")\n",
    "\n",
    "# Finally save builder\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Serving Client\n",
    "Showcase of a basic client that can use the models served by a tf-serving server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grpc.beta import implementations\n",
    "\n",
    "# reference local copy of Tensorflow Serving API Files\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, 'ext_libs'))\n",
    "import lib.predict_pb2 as predict_pb2\n",
    "import lib.prediction_service_pb2 as prediction_service_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='127.0.0.1'\n",
    "port=9000\n",
    "channel = implementations.insecure_channel(host, int(port))\n",
    "stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "# build request\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'rnn' # model name, as given to bazel script\n",
    "request.model_spec.signature_name = 'predict' # as defined in ModelBuilder\n",
    "\n",
    "# define inputs\n",
    "x = X_train[0]\n",
    "x_tensor = tf.contrib.util.make_tensor_proto(x, dtype=tf.float32, shape=(1, 20,))\n",
    "request.inputs['inputs'].CopyFrom(x_tensor)\n",
    "\n",
    "# call prediction on the server\n",
    "result = stub.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output from server response\n",
    "outputs = result.outputs['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract response-tensor shape\n",
    "tensor_shape = outputs.tensor_shape\n",
    "tensor_shape = [dim.size for dim in tensor_shape.dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape list of float to given shape\n",
    "res_tensor = np.array(outputs.float_val).reshape(tensor_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Serving Client\n",
    "We rely on the implementation of the tensorflow serving client as provided in this repository. It is just a refactoring of the previous code for better modularity and reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.servingClient import ServingClient\n",
    "from model.textGenerator import TextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate text generator object \n",
    "# with info about which model to use and related configs\n",
    "text_gen = TextGenerator('/Users/amartinelli/Documents/models/models.ini',\n",
    "             'darwin_tf', \n",
    "            'standard_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "text_gen.generate(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensor-flow]",
   "language": "python",
   "name": "conda-env-tensor-flow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "888px",
    "left": "0px",
    "right": "1691px",
    "top": "107px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
